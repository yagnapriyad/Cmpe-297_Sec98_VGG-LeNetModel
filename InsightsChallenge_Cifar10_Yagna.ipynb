{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InsightsChallenge_Cifar10_Yagna.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0YVyjYjHMA7",
        "colab_type": "text"
      },
      "source": [
        "### Developed image classification of cifar10 dataset using pytorch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ZOnT943DUB",
        "colab_type": "code",
        "outputId": "8fcea485-80e3-4cff-c13a-fd1893ead14c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzT9x9lr3X8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Import dependencies\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import csv\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-haBPlAfHFRk",
        "colab_type": "text"
      },
      "source": [
        "### Defining the hyperparameters for model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88gUgxJv3tVc",
        "colab_type": "code",
        "outputId": "6e22ad98-479a-4e84-c8f0-b6026b06ac78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "cellView": "both"
      },
      "source": [
        "#@title Set hyperparameters\n",
        "\n",
        "num_classes = 20   # number of output classes discrete range [0,9]\n",
        "num_epochs  = 30    # number of times the entire dataset is presented to the model\n",
        "batch_size  = 64   # the size of input data took for one iteration\n",
        "lr          = 0.002 # size of step\n",
        "momentum = 0.9\n",
        "weight_decay=5e-4\n",
        "nesterov=True\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkdnbavr3wC6",
        "colab_type": "code",
        "outputId": "03edfb6b-8f7a-47ad-935b-bc73ba25f47d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#@title Download CIFAR-10 and set up dataloaders\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "train_data = dsets.CIFAR10(root='./data',\n",
        "                           train=True,\n",
        "                           transform=transform, download=True)\n",
        "\n",
        "test_data = dsets.CIFAR10(root='./data', train=False,\n",
        "                          transform=transform, download=True)\n",
        "\n",
        "train_gen = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                        batch_size=batch_size,\n",
        "                                        shuffle=True, num_workers=2)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                       batch_size=batch_size, \n",
        "                                       shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFtrGcqVGpUP",
        "colab_type": "text"
      },
      "source": [
        "### Defining the model.\n",
        " Model is created with 3 convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwTPHBPs3zjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Define the model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(Net, self).__init__()\n",
        "    ############################################################################\n",
        "    # TODO (optional): Define modules you might need for your neural network.  #\n",
        "    ############################################################################\n",
        "    self.conv_layer = nn.Sequential(\n",
        "      # Conv Layer block 1\n",
        "      nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "      # Conv Layer block 2\n",
        "      nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Dropout2d(p=0.05),\n",
        "\n",
        "      # Conv Layer block 3\n",
        "      nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "      nn.BatchNorm2d(256),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "    )\n",
        "\n",
        "    self.fc_layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(4096, 1024),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "\n",
        "    ############################################################################\n",
        "    #                              END OF YOUR CODE                            #\n",
        "    ############################################################################\n",
        "\n",
        "  def forward(self, x):\n",
        "    ############################################################################\n",
        "    # TODO: Perform a forward pass through your network from inputs `x` of     #\n",
        "    # shape (batch_size x 32 x 32) to output class scores `out` of shape       #\n",
        "    # (batch_size x 10). You can use modules defined in the constructor above. #\n",
        "    ############################################################################\n",
        "    # conv layers\n",
        "    x = self.conv_layer(x)\n",
        "    \n",
        "    # flatten\n",
        "    x = x.view(x.size(0), -1)\n",
        "    \n",
        "    # fc layer\n",
        "    x = self.fc_layer(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "    ############################################################################\n",
        "    #                              END OF YOUR CODE                            #\n",
        "    ############################################################################\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8jCEKxZ352S",
        "colab_type": "code",
        "outputId": "13d7a2e6-bdc7-4fb3-bfaa-cf9e97702d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "#@title Build the model\n",
        "\n",
        "net = Net(num_classes)\n",
        "net.to(device)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv_layer): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (12): Dropout2d(p=0.05, inplace=False)\n",
              "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc_layer): Sequential(\n",
              "    (0): Dropout(p=0.1, inplace=False)\n",
              "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrdNVaPg36qJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Define loss function & optimizer\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4, nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OtWcI2X3-6w",
        "colab_type": "code",
        "outputId": "a309b692-ee83-4039-b6f5-0c810078655f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Train model\n",
        "\n",
        "net.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_gen):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(images)\n",
        "\n",
        "    loss = loss_function(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                 %(epoch+1, num_epochs, i+1, len(train_data)//batch_size,\n",
        "                   loss.item()))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Step [100/781], Loss: 2.0894\n",
            "Epoch [1/30], Step [200/781], Loss: 1.6813\n",
            "Epoch [1/30], Step [300/781], Loss: 1.5420\n",
            "Epoch [1/30], Step [400/781], Loss: 1.3253\n",
            "Epoch [1/30], Step [500/781], Loss: 1.3803\n",
            "Epoch [1/30], Step [600/781], Loss: 1.2281\n",
            "Epoch [1/30], Step [700/781], Loss: 1.1619\n",
            "Epoch [2/30], Step [100/781], Loss: 1.0442\n",
            "Epoch [2/30], Step [200/781], Loss: 0.8567\n",
            "Epoch [2/30], Step [300/781], Loss: 0.8320\n",
            "Epoch [2/30], Step [400/781], Loss: 0.7931\n",
            "Epoch [2/30], Step [500/781], Loss: 0.8361\n",
            "Epoch [2/30], Step [600/781], Loss: 0.7413\n",
            "Epoch [2/30], Step [700/781], Loss: 0.7732\n",
            "Epoch [3/30], Step [100/781], Loss: 0.7789\n",
            "Epoch [3/30], Step [200/781], Loss: 0.8998\n",
            "Epoch [3/30], Step [300/781], Loss: 0.5946\n",
            "Epoch [3/30], Step [400/781], Loss: 0.7189\n",
            "Epoch [3/30], Step [500/781], Loss: 0.6387\n",
            "Epoch [3/30], Step [600/781], Loss: 0.6426\n",
            "Epoch [3/30], Step [700/781], Loss: 0.8161\n",
            "Epoch [4/30], Step [100/781], Loss: 0.5312\n",
            "Epoch [4/30], Step [200/781], Loss: 0.6191\n",
            "Epoch [4/30], Step [300/781], Loss: 0.6823\n",
            "Epoch [4/30], Step [400/781], Loss: 0.4580\n",
            "Epoch [4/30], Step [500/781], Loss: 0.4654\n",
            "Epoch [4/30], Step [600/781], Loss: 0.5485\n",
            "Epoch [4/30], Step [700/781], Loss: 0.3486\n",
            "Epoch [5/30], Step [100/781], Loss: 0.4660\n",
            "Epoch [5/30], Step [200/781], Loss: 0.3190\n",
            "Epoch [5/30], Step [300/781], Loss: 0.3572\n",
            "Epoch [5/30], Step [400/781], Loss: 0.7126\n",
            "Epoch [5/30], Step [500/781], Loss: 0.4864\n",
            "Epoch [5/30], Step [600/781], Loss: 0.4857\n",
            "Epoch [5/30], Step [700/781], Loss: 0.5624\n",
            "Epoch [6/30], Step [100/781], Loss: 0.3405\n",
            "Epoch [6/30], Step [200/781], Loss: 0.4462\n",
            "Epoch [6/30], Step [300/781], Loss: 0.6452\n",
            "Epoch [6/30], Step [400/781], Loss: 0.4122\n",
            "Epoch [6/30], Step [500/781], Loss: 0.4073\n",
            "Epoch [6/30], Step [600/781], Loss: 0.6002\n",
            "Epoch [6/30], Step [700/781], Loss: 0.5557\n",
            "Epoch [7/30], Step [100/781], Loss: 0.3786\n",
            "Epoch [7/30], Step [200/781], Loss: 0.3880\n",
            "Epoch [7/30], Step [300/781], Loss: 0.3484\n",
            "Epoch [7/30], Step [400/781], Loss: 0.2935\n",
            "Epoch [7/30], Step [500/781], Loss: 0.2343\n",
            "Epoch [7/30], Step [600/781], Loss: 0.2539\n",
            "Epoch [7/30], Step [700/781], Loss: 0.2953\n",
            "Epoch [8/30], Step [100/781], Loss: 0.3955\n",
            "Epoch [8/30], Step [200/781], Loss: 0.2857\n",
            "Epoch [8/30], Step [300/781], Loss: 0.2375\n",
            "Epoch [8/30], Step [400/781], Loss: 0.1707\n",
            "Epoch [8/30], Step [500/781], Loss: 0.2741\n",
            "Epoch [8/30], Step [600/781], Loss: 0.2768\n",
            "Epoch [8/30], Step [700/781], Loss: 0.2519\n",
            "Epoch [9/30], Step [100/781], Loss: 0.2310\n",
            "Epoch [9/30], Step [200/781], Loss: 0.2006\n",
            "Epoch [9/30], Step [300/781], Loss: 0.3680\n",
            "Epoch [9/30], Step [400/781], Loss: 0.2552\n",
            "Epoch [9/30], Step [500/781], Loss: 0.2162\n",
            "Epoch [9/30], Step [600/781], Loss: 0.2422\n",
            "Epoch [9/30], Step [700/781], Loss: 0.3216\n",
            "Epoch [10/30], Step [100/781], Loss: 0.2174\n",
            "Epoch [10/30], Step [200/781], Loss: 0.2261\n",
            "Epoch [10/30], Step [300/781], Loss: 0.3010\n",
            "Epoch [10/30], Step [400/781], Loss: 0.4861\n",
            "Epoch [10/30], Step [500/781], Loss: 0.2309\n",
            "Epoch [10/30], Step [600/781], Loss: 0.2621\n",
            "Epoch [10/30], Step [700/781], Loss: 0.1073\n",
            "Epoch [11/30], Step [100/781], Loss: 0.2134\n",
            "Epoch [11/30], Step [200/781], Loss: 0.1601\n",
            "Epoch [11/30], Step [300/781], Loss: 0.1847\n",
            "Epoch [11/30], Step [400/781], Loss: 0.1860\n",
            "Epoch [11/30], Step [500/781], Loss: 0.2214\n",
            "Epoch [11/30], Step [600/781], Loss: 0.1453\n",
            "Epoch [11/30], Step [700/781], Loss: 0.1588\n",
            "Epoch [12/30], Step [100/781], Loss: 0.1514\n",
            "Epoch [12/30], Step [200/781], Loss: 0.0843\n",
            "Epoch [12/30], Step [300/781], Loss: 0.1144\n",
            "Epoch [12/30], Step [400/781], Loss: 0.0912\n",
            "Epoch [12/30], Step [500/781], Loss: 0.0862\n",
            "Epoch [12/30], Step [600/781], Loss: 0.0985\n",
            "Epoch [12/30], Step [700/781], Loss: 0.2458\n",
            "Epoch [13/30], Step [100/781], Loss: 0.2230\n",
            "Epoch [13/30], Step [200/781], Loss: 0.0809\n",
            "Epoch [13/30], Step [300/781], Loss: 0.0550\n",
            "Epoch [13/30], Step [400/781], Loss: 0.0575\n",
            "Epoch [13/30], Step [500/781], Loss: 0.1669\n",
            "Epoch [13/30], Step [600/781], Loss: 0.0540\n",
            "Epoch [13/30], Step [700/781], Loss: 0.2493\n",
            "Epoch [14/30], Step [100/781], Loss: 0.0380\n",
            "Epoch [14/30], Step [200/781], Loss: 0.0802\n",
            "Epoch [14/30], Step [300/781], Loss: 0.0890\n",
            "Epoch [14/30], Step [400/781], Loss: 0.1148\n",
            "Epoch [14/30], Step [500/781], Loss: 0.0892\n",
            "Epoch [14/30], Step [600/781], Loss: 0.0691\n",
            "Epoch [14/30], Step [700/781], Loss: 0.0280\n",
            "Epoch [15/30], Step [100/781], Loss: 0.1399\n",
            "Epoch [15/30], Step [200/781], Loss: 0.0702\n",
            "Epoch [15/30], Step [300/781], Loss: 0.0650\n",
            "Epoch [15/30], Step [400/781], Loss: 0.0433\n",
            "Epoch [15/30], Step [500/781], Loss: 0.0637\n",
            "Epoch [15/30], Step [600/781], Loss: 0.1252\n",
            "Epoch [15/30], Step [700/781], Loss: 0.1435\n",
            "Epoch [16/30], Step [100/781], Loss: 0.0436\n",
            "Epoch [16/30], Step [200/781], Loss: 0.0556\n",
            "Epoch [16/30], Step [300/781], Loss: 0.1779\n",
            "Epoch [16/30], Step [400/781], Loss: 0.0663\n",
            "Epoch [16/30], Step [500/781], Loss: 0.0382\n",
            "Epoch [16/30], Step [600/781], Loss: 0.1084\n",
            "Epoch [16/30], Step [700/781], Loss: 0.0171\n",
            "Epoch [17/30], Step [100/781], Loss: 0.0129\n",
            "Epoch [17/30], Step [200/781], Loss: 0.0908\n",
            "Epoch [17/30], Step [300/781], Loss: 0.0797\n",
            "Epoch [17/30], Step [400/781], Loss: 0.0452\n",
            "Epoch [17/30], Step [500/781], Loss: 0.1477\n",
            "Epoch [17/30], Step [600/781], Loss: 0.1186\n",
            "Epoch [17/30], Step [700/781], Loss: 0.0688\n",
            "Epoch [18/30], Step [100/781], Loss: 0.1405\n",
            "Epoch [18/30], Step [200/781], Loss: 0.3250\n",
            "Epoch [18/30], Step [300/781], Loss: 0.0675\n",
            "Epoch [18/30], Step [400/781], Loss: 0.0874\n",
            "Epoch [18/30], Step [500/781], Loss: 0.0602\n",
            "Epoch [18/30], Step [600/781], Loss: 0.0542\n",
            "Epoch [18/30], Step [700/781], Loss: 0.1021\n",
            "Epoch [19/30], Step [100/781], Loss: 0.0347\n",
            "Epoch [19/30], Step [200/781], Loss: 0.1055\n",
            "Epoch [19/30], Step [300/781], Loss: 0.0434\n",
            "Epoch [19/30], Step [400/781], Loss: 0.0999\n",
            "Epoch [19/30], Step [500/781], Loss: 0.0346\n",
            "Epoch [19/30], Step [600/781], Loss: 0.0284\n",
            "Epoch [19/30], Step [700/781], Loss: 0.0783\n",
            "Epoch [20/30], Step [100/781], Loss: 0.0203\n",
            "Epoch [20/30], Step [200/781], Loss: 0.0109\n",
            "Epoch [20/30], Step [300/781], Loss: 0.0582\n",
            "Epoch [20/30], Step [400/781], Loss: 0.0128\n",
            "Epoch [20/30], Step [500/781], Loss: 0.1016\n",
            "Epoch [20/30], Step [600/781], Loss: 0.0580\n",
            "Epoch [20/30], Step [700/781], Loss: 0.0214\n",
            "Epoch [21/30], Step [100/781], Loss: 0.0338\n",
            "Epoch [21/30], Step [200/781], Loss: 0.0390\n",
            "Epoch [21/30], Step [300/781], Loss: 0.0076\n",
            "Epoch [21/30], Step [400/781], Loss: 0.0215\n",
            "Epoch [21/30], Step [500/781], Loss: 0.0313\n",
            "Epoch [21/30], Step [600/781], Loss: 0.0222\n",
            "Epoch [21/30], Step [700/781], Loss: 0.0551\n",
            "Epoch [22/30], Step [100/781], Loss: 0.0127\n",
            "Epoch [22/30], Step [200/781], Loss: 0.0417\n",
            "Epoch [22/30], Step [300/781], Loss: 0.0410\n",
            "Epoch [22/30], Step [400/781], Loss: 0.0081\n",
            "Epoch [22/30], Step [500/781], Loss: 0.0262\n",
            "Epoch [22/30], Step [600/781], Loss: 0.0362\n",
            "Epoch [22/30], Step [700/781], Loss: 0.0381\n",
            "Epoch [23/30], Step [100/781], Loss: 0.0313\n",
            "Epoch [23/30], Step [200/781], Loss: 0.0495\n",
            "Epoch [23/30], Step [300/781], Loss: 0.0453\n",
            "Epoch [23/30], Step [400/781], Loss: 0.0307\n",
            "Epoch [23/30], Step [500/781], Loss: 0.0135\n",
            "Epoch [23/30], Step [600/781], Loss: 0.0068\n",
            "Epoch [23/30], Step [700/781], Loss: 0.0153\n",
            "Epoch [24/30], Step [100/781], Loss: 0.0417\n",
            "Epoch [24/30], Step [200/781], Loss: 0.0097\n",
            "Epoch [24/30], Step [300/781], Loss: 0.0469\n",
            "Epoch [24/30], Step [400/781], Loss: 0.0942\n",
            "Epoch [24/30], Step [500/781], Loss: 0.0036\n",
            "Epoch [24/30], Step [600/781], Loss: 0.0084\n",
            "Epoch [24/30], Step [700/781], Loss: 0.0334\n",
            "Epoch [25/30], Step [100/781], Loss: 0.0399\n",
            "Epoch [25/30], Step [200/781], Loss: 0.0330\n",
            "Epoch [25/30], Step [300/781], Loss: 0.0130\n",
            "Epoch [25/30], Step [400/781], Loss: 0.0143\n",
            "Epoch [25/30], Step [500/781], Loss: 0.0299\n",
            "Epoch [25/30], Step [600/781], Loss: 0.0414\n",
            "Epoch [25/30], Step [700/781], Loss: 0.0644\n",
            "Epoch [26/30], Step [100/781], Loss: 0.0158\n",
            "Epoch [26/30], Step [200/781], Loss: 0.0216\n",
            "Epoch [26/30], Step [300/781], Loss: 0.0368\n",
            "Epoch [26/30], Step [400/781], Loss: 0.0088\n",
            "Epoch [26/30], Step [500/781], Loss: 0.0060\n",
            "Epoch [26/30], Step [600/781], Loss: 0.0032\n",
            "Epoch [26/30], Step [700/781], Loss: 0.0059\n",
            "Epoch [27/30], Step [100/781], Loss: 0.0203\n",
            "Epoch [27/30], Step [200/781], Loss: 0.0660\n",
            "Epoch [27/30], Step [300/781], Loss: 0.0147\n",
            "Epoch [27/30], Step [400/781], Loss: 0.0296\n",
            "Epoch [27/30], Step [500/781], Loss: 0.0127\n",
            "Epoch [27/30], Step [600/781], Loss: 0.0149\n",
            "Epoch [27/30], Step [700/781], Loss: 0.0114\n",
            "Epoch [28/30], Step [100/781], Loss: 0.0284\n",
            "Epoch [28/30], Step [200/781], Loss: 0.0247\n",
            "Epoch [28/30], Step [300/781], Loss: 0.0068\n",
            "Epoch [28/30], Step [400/781], Loss: 0.0043\n",
            "Epoch [28/30], Step [500/781], Loss: 0.0189\n",
            "Epoch [28/30], Step [600/781], Loss: 0.0165\n",
            "Epoch [28/30], Step [700/781], Loss: 0.0127\n",
            "Epoch [29/30], Step [100/781], Loss: 0.0084\n",
            "Epoch [29/30], Step [200/781], Loss: 0.0101\n",
            "Epoch [29/30], Step [300/781], Loss: 0.0137\n",
            "Epoch [29/30], Step [400/781], Loss: 0.0075\n",
            "Epoch [29/30], Step [500/781], Loss: 0.0134\n",
            "Epoch [29/30], Step [600/781], Loss: 0.0094\n",
            "Epoch [29/30], Step [700/781], Loss: 0.0056\n",
            "Epoch [30/30], Step [100/781], Loss: 0.0126\n",
            "Epoch [30/30], Step [200/781], Loss: 0.0055\n",
            "Epoch [30/30], Step [300/781], Loss: 0.0476\n",
            "Epoch [30/30], Step [400/781], Loss: 0.0133\n",
            "Epoch [30/30], Step [500/781], Loss: 0.0029\n",
            "Epoch [30/30], Step [600/781], Loss: 0.0025\n",
            "Epoch [30/30], Step [700/781], Loss: 0.0081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAl_IxGGL4s",
        "colab_type": "text"
      },
      "source": [
        "### Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbD2hP4GeGrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYFOP2AxfveI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ef0496c-07ce-4c40-f7a4-87276dfb5e96"
      },
      "source": [
        "net = Net(10)\n",
        "net.cuda()\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cLQIUUmGQkH",
        "colab_type": "text"
      },
      "source": [
        "### Testing the accuracy of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pG3S2Vb6eGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "052addad-1704-4ab0-a4bf-a71cf46bcf0b"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in test_gen:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 91 %\n",
            "Accuracy of   car : 96 %\n",
            "Accuracy of  bird : 81 %\n",
            "Accuracy of   cat : 63 %\n",
            "Accuracy of  deer : 72 %\n",
            "Accuracy of   dog : 81 %\n",
            "Accuracy of  frog : 85 %\n",
            "Accuracy of horse : 84 %\n",
            "Accuracy of  ship : 91 %\n",
            "Accuracy of truck : 91 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyFn0-zpGV-X",
        "colab_type": "text"
      },
      "source": [
        "### Write predictions to caliper_cifar10_test_predictions.csv and downloading the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JifuBfwC4EUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run model on test set and save predictions\n",
        "\n",
        "net.eval()\n",
        "predictions = []\n",
        "\n",
        "for images, _ in test_gen:\n",
        "  images = images.to(device)\n",
        "\n",
        "  output = net(images)\n",
        "  _, predicted = torch.max(output, 1)\n",
        "    \n",
        "  img_id_start = len(predictions) + 1\n",
        "  predictions += [{'image_id': img_id_start + x,\n",
        "                   'label': predicted[x].item()} for x in range(len(predicted))]\n",
        "\n",
        "with open('gdrive/My Drive/Data/caliper_cifar10_test_predictions.csv', mode='w') as preds_file:\n",
        "    writer = csv.writer(preds_file, delimiter=',')\n",
        "    writer.writerow(['id','label'])\n",
        "    for el in predictions:\n",
        "        writer.writerow([el['image_id'], el['label']])\n",
        "    \n",
        "files.download('gdrive/My Drive/Data/caliper_cifar10_test_predictions.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN4IfXSz4EbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3D-MZqT4EeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTlDx6ZC4ERj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}